{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Background\n",
    "In the field of machine learning, hyperparameter optimization is a critical task that has been studied by numerous researchers. One major challenge in HPO problems is predicting the predictive performance for unknown hyperparameters. In this article, we investigate whether PS-Forest is useful for the HPO performance prediction task and compare it to other algorithms such as XGBoost and Gaussian Process."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Protocol\n",
    "In our experiment, we first prepare experimental data by randomly sampling 1000 hyperparameter configurations from the HPO-Bench search space, and then we compute the validation score using the HPO-Bench training protocol. The real-world dataset used for training XGBoost is \"kr-vs-kp\". Our goal is to see if our algorithm can accurately predict the performance of new hyperparameters based on previous hyperparameter optimization data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "data = pd.read_csv('data/XGB_HPO_data.csv', index_col=0)\n",
    "for c in ['eta', 'min_child_weight', 'reg_alpha', 'reg_lambda']:\n",
    "    data[c] = np.log(data[c])\n",
    "\n",
    "x = np.array(data[['colsample_bylevel', 'colsample_bytree', 'eta', 'max_depth',\n",
    "                   'min_child_weight', 'reg_alpha',\n",
    "                   'reg_lambda', 'subsample_per_it']])\n",
    "y = np.array(data['function_value'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from evolutionary_forest.forest import EvolutionaryForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=0)\n",
    "basic_primitive = 'add,subtract,multiply,analytical_quotient,protect_sqrt,analytical_loge,maximum'\n",
    "forest = EvolutionaryForestRegressor(\n",
    "    max_height=3, normalize=False, select='AutomaticLexicase', boost_size=100,\n",
    "    basic_primitives=basic_primitive, mutation_scheme='EDA-Terminal-PM',\n",
    "    semantic_diversity='GreedySelection-Resampling', initial_tree_size='1-2',\n",
    "    cross_pb=0.9, mutation_pb=0.1, gene_num=10, n_gen=30,\n",
    "    n_pop=200, base_learner='Fast-RidgeDT'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Important Feature Visualization\n",
    "First, we train a PS-Forest and plot the feature importance map. Based on this feature importance map, we discover that the maximum value of the minimum child weight (MCW) and the L1 regularization (RA) term is critical in the XGBoost performance prediction task. This is an intriguing discovery because it implies that changing either the MCW or the L1 regularization term could have a similar effect on XGBoost performance. Yet, this is also reasonable because increasing either the MCW or the L1 regularization term reduces the complexity of the final model. However, changing both terms has no effect on the model's complexity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from evolutionary_forest.utils import plot_feature_importance\n",
    "from evolutionary_forest.utils import get_feature_importance\n",
    "\n",
    "forest.fit(x, y)\n",
    "\n",
    "feature_importance_dict = get_feature_importance(forest, simple_version=True)\n",
    "data_columns = ['CBL', 'CBT', 'ETA', 'MD',\n",
    "                'MCW', 'RA', 'RL', 'SS']\n",
    "for k, v in list(feature_importance_dict.items()):\n",
    "    del feature_importance_dict[k]\n",
    "    for i in range(len(data_columns) - 1, -1, -1):\n",
    "        k = k.replace(\"X_{%d}\" % i, data_columns[i])\n",
    "    feature_importance_dict[k] = v\n",
    "plot_feature_importance(feature_importance_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison with Other Algorithms\n",
    "The preceding result demonstrates that PS-Forest can learn some useful patterns from training data. In this section, we will compare PS-Forest to other cutting-edge algorithms, including XGBoost and Gaussian Process. To simulate the data scarcity scenario in HPO tasks, we perform ten-fold cross-validation, using one-fold as training data and the remaining as testing data, and then plot the distribution of cross-validation score in the figure below. According to the experimental results, PS-Forest is the only learning method capable of achieving a cross-validation score greater than 0.8. This suggests that PS-Forest is a useful tool for HPO performance prediction tasks, and that it may have a wide range of applications in the future."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.utils.validation import _num_samples\n",
    "from evolutionary_forest.model.PLTree import RidgeDT\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.utils import indexable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ParameterGrid, KFold\n",
    "\n",
    "\n",
    "class SmallSampleKFold(KFold):\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        if self.n_splits > n_samples:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of splits n_splits={0} greater\"\n",
    "                 \" than the number of samples: n_samples={1}.\")\n",
    "                .format(self.n_splits, n_samples))\n",
    "\n",
    "        for train, test in super().split(X, y, groups):\n",
    "            yield test, train\n",
    "\n",
    "\n",
    "model_list = {\n",
    "    'RPL-Tree': RidgeDT(decision_tree_count=2),\n",
    "    'RPL-Forest': BaggingRegressor(RidgeDT(decision_tree_count=2), n_estimators=100, n_jobs=1),\n",
    "    'Ridge': Ridge(),\n",
    "    # 'KR': KernelRidge(kernel='poly'),\n",
    "    'KNN': Pipeline([\n",
    "        ('Scaler', StandardScaler()),\n",
    "        ('KNN', KNeighborsRegressor())\n",
    "    ]),\n",
    "    'GPR': Pipeline([\n",
    "        ('Scaler', StandardScaler()),\n",
    "        ('GP', GaussianProcessRegressor(kernel=Matern(), normalize_y=True))\n",
    "    ]),\n",
    "    'DT': DecisionTreeRegressor(),\n",
    "    'RF': RandomForestRegressor(n_estimators=100),\n",
    "    'ET': ExtraTreesRegressor(n_estimators=100),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100),\n",
    "    'GBDT': GradientBoostingRegressor(n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, n_jobs=1),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, n_jobs=1),\n",
    "    'PS-Forest': EvolutionaryForestRegressor(\n",
    "        max_height=3, normalize=False, select='AutomaticLexicase', boost_size=100,\n",
    "        basic_primitives=basic_primitive, mutation_scheme='EDA-Terminal-PM',\n",
    "        semantic_diversity='GreedySelection-Resampling', initial_tree_size='1-2',\n",
    "        cross_pb=0.9, mutation_pb=0.1, gene_num=10, n_gen=30,\n",
    "        n_pop=200, base_learner='Fast-RidgeDT'\n",
    "    )\n",
    "}\n",
    "\n",
    "all_score = []\n",
    "detailed_score = []\n",
    "mean_score = []\n",
    "for model in ['RPL-Tree', 'RPL-Forest', 'GPR', 'KNN', 'Ridge', 'DT',\n",
    "              'RF', 'ET', 'AdaBoost', 'GBDT', 'XGBoost', 'LightGBM', 'PS-Forest']:\n",
    "    # score = model_scoring(model, x_test, x_train, y_test, y_train)\n",
    "    score = cross_val_score(model_list[model], x, y, cv=SmallSampleKFold(10), n_jobs=-1)\n",
    "    all_score.append((model, np.mean(score), *score))\n",
    "    for s in score:\n",
    "        detailed_score.append((model, s))\n",
    "    mean_score.append((model, np.mean(score)))\n",
    "    print(model, score, np.mean(score))\n",
    "detailed_score = pd.DataFrame(detailed_score, columns=['Model', 'Score ($R^2$)'])\n",
    "mean_score = pd.DataFrame(mean_score, columns=['Model', 'Score'])\n",
    "print(pd.DataFrame(all_score))\n",
    "sns.set(style='whitegrid')\n",
    "sns.boxplot(x=\"Model\", y=\"Score ($R^2$)\", data=detailed_score, showfliers=True, palette='vlag', width=0.6)\n",
    "sns.scatterplot(x=\"Model\", y=\"Score\", data=mean_score, color='black', alpha=0.5, label='Mean Score')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
