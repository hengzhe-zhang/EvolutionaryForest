{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Introduction\n",
    "In the realm of traditional machine learning, constructing robust and meaningful features can substantially enhance the performance of the final model. Consequently, this notebook aims to showcase the capabilities of this package in automated feature construction. For the sake of simplicity, a problem from the scikit-learn package has been selected as the case study. This task, referred to as \"diabetes,\" aims to predict a quantitative measure of disease progression one year after the baseline. Initially, the data is divided into training and testing sets, followed by the separate training of a random forest and an evolutionary forest. Finally, these models are tested on the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T14:46:18.174117Z",
     "start_time": "2023-04-16T14:46:09.075383Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from evolutionary_forest.forest import EvolutionaryForestRegressor\n",
    "from evolutionary_forest.utils import get_feature_importance, plot_feature_importance, combine_features\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T14:47:20.867757Z",
     "start_time": "2023-04-16T14:46:18.178392Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Train Random Forest\n",
    "r = RandomForestRegressor()\n",
    "r.fit(x_train, y_train)\n",
    "print(r2_score(y_test, r.predict(x_test)))\n",
    "\n",
    "# Train Evolutionary Forest\n",
    "r = EvolutionaryForestRegressor(max_height=5, normalize=True, select='AutomaticLexicase',\n",
    "                                gene_num=10, boost_size=100, n_gen=20, n_pop=200, cross_pb=1,\n",
    "                                base_learner='Random-DT', verbose=True, n_process=1)\n",
    "r.fit(x_train, y_train)\n",
    "print(r2_score(y_test, r.predict(x_test)))"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Visualization\n",
    "Based on these results, it is evident that the evolutionary forest surpasses the traditional random forest in performance. However, we should not be content with merely having a superior model. In fact, a more significant objective is to acquire more explainable features, which can enhance the performance of numerous machine learning models. Consequently, in the subsequent section, we calculate feature importance based on impurity reduction and then rank all features according to their importance scores. For clarity, our package displays only the top 15 most important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T10:13:51.955032Z",
     "start_time": "2023-04-16T10:13:50.854192Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Calculate and plot feature importance\n",
    "feature_importance_dict = get_feature_importance(r)\n",
    "plot_feature_importance(feature_importance_dict)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the importance map, we can utilize these valuable features in the subsequent section and examine whether they can genuinely enhance the performance of an existing model. For simplicity, we have omitted the original features and retained only the constructed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T10:13:55.130411Z",
     "start_time": "2023-04-16T10:13:51.957023Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Synthesize new features\n",
    "code_importance_dict = get_feature_importance(r, simple_version=False)\n",
    "top_features = list(code_importance_dict.keys())[:len(code_importance_dict) // 2]\n",
    "new_train = combine_features(r, r.x_scaler.transform(x_train), top_features, only_new_features=True)\n",
    "new_test = combine_features(r, r.x_scaler.transform(x_test), top_features, only_new_features=True)\n",
    "\n",
    "# Train a new model on synthesized features\n",
    "new_r = RandomForestRegressor()\n",
    "new_r.fit(new_train, y_train)\n",
    "print(r2_score(y_test, new_r.predict(new_test)))"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Investigation of Feature Construction\n",
    "The results clearly indicate that the constructed features can effectively contribute to improved performance, highlighting the efficacy of these features. However, a more intriguing question is whether these features are limited to this specific model, or if they can be applied to other machine learning models as well. Therefore, in the subsequent section, we aim to determine if the features constructed by EF can enhance the performance of existing state-of-the-art machine learning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "regressor_list = ['RF', 'ET', 'AdaBoost', 'GBDT', 'DART', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "\n",
    "# Normalize training and testing data\n",
    "x_train, x_test = r.x_scaler.transform(x_train), r.x_scaler.transform(x_test)\n",
    "\n",
    "# Get feature importance and select top features\n",
    "code_importance_dict = get_feature_importance(r, simple_version=False)\n",
    "top_features = list(code_importance_dict.keys())[:len(code_importance_dict.keys()) // 2]\n",
    "\n",
    "# Replace the original dataset with new features\n",
    "new_train = combine_features(r, x_train, top_features, only_new_features=True)\n",
    "new_test = combine_features(r, x_test, top_features, only_new_features=True)\n",
    "\n",
    "# Define regressors\n",
    "regressor_dict = {\n",
    "    'RF': RandomForestRegressor(n_estimators=200, n_jobs=-1),\n",
    "    'ET': ExtraTreesRegressor(n_estimators=200, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=200),\n",
    "    'GBDT': GradientBoostingRegressor(n_estimators=200),\n",
    "    'DART': LGBMRegressor(n_jobs=1, n_estimators=200, boosting_type='dart',\n",
    "                          xgboost_dart_mode=True),\n",
    "    'XGBoost': XGBRegressor(n_jobs=1, n_estimators=200),\n",
    "    'LightGBM': LGBMRegressor(n_jobs=1, n_estimators=200),\n",
    "    'CatBoost': CatBoostRegressor(n_estimators=200, thread_count=1,\n",
    "                                  verbose=False, allow_writing_files=False),\n",
    "}\n",
    "\n",
    "scores_base = []\n",
    "scores_enhanced = []\n",
    "\n",
    "# Train and evaluate regressors on original and new features\n",
    "for regr in regressor_list:\n",
    "    regressor = regressor_dict[regr]\n",
    "\n",
    "    # Train on original features\n",
    "    regressor.fit(x_train, y_train)\n",
    "    score = r2_score(y_test, regressor.predict(x_test))\n",
    "    scores_base.append((regr, score))\n",
    "\n",
    "    # Train on new features\n",
    "    regressor.fit(new_train, y_train)\n",
    "    score = r2_score(y_test, regressor.predict(new_test))\n",
    "    scores_enhanced.append((regr, score))\n",
    "\n",
    "# Calculate mean scores for each algorithm\n",
    "base_frame = pd.DataFrame(scores_base, columns=['algorithm', 'score'])\n",
    "print(base_frame.groupby(['algorithm']).mean())\n",
    "\n",
    "improved_frame = pd.DataFrame(scores_enhanced, columns=['algorithm', 'score'])\n",
    "print(improved_frame.groupby(['algorithm']).mean())\n",
    "\n",
    "# Calculate the improvement in scores\n",
    "base_score = base_frame.groupby(['algorithm']).mean()\n",
    "improved_score = improved_frame.groupby(['algorithm']).mean()\n",
    "print(improved_score - base_score)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement Visualization\n",
    "Based on the experimental results, it can be concluded that constructive features enhance the performance of all models, with specific improvements seen in GBDT and XGBoost models. Therefore, we can assert that our method can function not only as an effective regression method for inducing a powerful regression model but also as a feature construction method that generates interpretable features and enhances the performance of existing machine learning systems. The improvement caused by constructed features is illustrated in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot scores for original features\n",
    "ax.bar(base_score.index, base_score['score'], width, label='Original Features')\n",
    "\n",
    "# Calculate score differences\n",
    "difference = improved_score['score'] - base_score['score']\n",
    "\n",
    "# Choose bar colors based on score improvement\n",
    "colors = np.where(difference > 0, 'r', 'y')\n",
    "\n",
    "# Plot score differences for constructed features\n",
    "ax.bar(base_score.index, difference, width, bottom=base_score['score'],\n",
    "       label='Constructed Features', color=colors)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_ylabel('Score ($R^2$)')\n",
    "ax.set_title('Effect of Feature Construction')\n",
    "ax.legend()\n",
    "\n",
    "# Format the x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we have demonstrated that our method can uncover valuable features that can enhance the performance of existing machine learning systems. However, it should be noted that even if the discovered features improve the validation score, there is a risk of overfitting. Hence, in real-world applications, it is advisable to scrutinize all utilized features to ensure the efficacy of the newly constructed model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
