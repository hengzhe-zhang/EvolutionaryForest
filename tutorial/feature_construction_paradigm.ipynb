{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In the machine learning domain, feature construction is always regarded as a critical component of a machine learning pipeline. Just as the deep neural network improves the performance of the traditional neural network through constructed features, if we can construct some powerful features, we may be able to significantly improve the learning capability of existing simple machine learning algorithms. In fact, in most ML competitions, feature engineering is always regarded as a critical point in winning the competition. Such features, however, necessitate a significant amount of effort on the part of domain experts. As a result, automated feature construction has become a hot topic in recent years. In general, we can generate several features using different methods, such as evolutionary algorithms or random search, and then select a few of them to use on the new machine learning system. In such a process, it is obvious that determining whether a feature is good or bad is a critical issue. As a result of this problem, most feature construction methods can be classified into three types: wrapper-based feature construction, filter-based feature construction, and embedded feature construction.\n",
    "The difference between these three methods is depicted in the figure below. The first is the wrapper-based feature construction method. It is obvious that this method is based on a specific machine learning algorithm. To be more specific, for a set of features, we must build a machine learning model based on those features and then use the validation score as the score of those features. The second method, the filter-based construction method, does not rely on any machine learning algorithm. In contrast, it estimates the efficacy of each feature using a statistical measure, such as the Pearson correlation between that feature and the response variable. Finally, the third method, embedded feature construction, is only applicable to some machine learning algorithms, such as decision tree based methods, which can provide feature importance. As a result, we can use the feature's important value to select critical features.\n",
    "\n",
    "These three methods each have their own set of benefits and drawbacks. The wrapper-based feature construction method, for example, is the most reliable method because it directly optimizes the validation score on a specific algorithm. The disadvantage is that it is extremely slow, especially when we need to construct some features for some expensive machine learning algorithms, such as neural networks or gradient boosted decision trees. In contrast, the filter-based construction method is quick because we don't need to train any machine learning algorithms during the scoring process. However, because it does not rely on any machine learning algorithms, the obtained result may not be applicable to the target algorithm. In terms of the last one, the embedded features construction method can, in fact, collaborate with the wrapper-based feature construction method. As a result, we will concentrate on comparing the first two methods in this article."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this article, I want to clear up a common misconception: unlike most people, I do not believe that the wrapper-based feature construction method is significantly slower than the filter-based feature construction method. In reality, the difference is not that great, and it should be within an order of magnitude. To demonstrate such an assumption, we run a scenario that evaluates a set of five features using Spearman correlation and a decision tree. As shown below, I repeated the experiment 1000 times to ensure the reliability of the experimental results. We can see that the Spearman correlation method must take two seconds to complete this process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from scipy.stats import spearmanr\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "for i in range(1000):\n",
    "    for j in range(5):\n",
    "        pseudo_feature = np.random.rand(len(y_train))\n",
    "        spearmanr(pseudo_feature, y_train)\n",
    "print('time', time.time() - st)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In comparison, if we build a decision tree based on the construction features and use the cross validation loss as the score of those features, the surprising thing is that we don't have to spend a lot of time on it. In reality, we only need to spend three more times to complete this task. The decision tree algorithm is, in fact, a lightning-fast algorithm. In theory, the decision tree's depth should be O(logN), where N represents the amount of data. Given such a fast algorithm, it's debatable whether we really need a less expensive but unreliable feature score estimation method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "st = time.time()\n",
    "for i in range(1000):\n",
    "    pseudo_feature = np.random.rand(len(y_train), 5)\n",
    "    tree = DecisionTreeRegressor()\n",
    "    cross_val_score(tree, pseudo_feature, y_train, cv=5)\n",
    "print('time', time.time() - st)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To investigate the problem further, namely whether it is worthwhile to use three more times to find the useful features, we chose two frameworks in the genetic programming-based feature construction domain to validate the effectiveness of these two paradigms. The evolutionary forest is the first. It is a new framework based on a wrapper-based feature construction paradigm. This algorithm, like the illustrative example, evaluates features using a decision tree. As a result, the obtained features can be applied directly to tree-based algorithms. We found that it will take 27 seconds to obtain the results based on the experimental results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from evolutionary_forest.forest import EvolutionaryForestRegressor\n",
    "\n",
    "st = time.time()\n",
    "r = EvolutionaryForestRegressor(max_height=8, normalize=True, select='AutomaticLexicase',\n",
    "                                mutation_scheme='weight-plus-cross-global',\n",
    "                                gene_num=10, boost_size=100, n_gen=20, base_learner='DT',\n",
    "                                verbose=True)\n",
    "r.fit(x_train, y_train)\n",
    "print('time', time.time() - st)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second is gplearn, which is a well-known genetic programming package. Unlike the previous package, this one employs the Spearman correlation coefficient to determine whether or not a feature is promising. As a result, this package has the advantage of being faster than the wrapper-based feature construction method. However, it is difficult to say whether the built feature is applicable to the target machine learning algorithm. Based on the experimental results, we found that it will take 8 seconds to obtain the results, demonstrating that this paradigm is significantly faster than the wrapper-based one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from gplearn.genetic import SymbolicTransformer\n",
    "\n",
    "st = time.time()\n",
    "s = SymbolicTransformer(population_size=500, generations=20, hall_of_fame=500, n_components=50, metric='spearman',\n",
    "                        verbose=1)\n",
    "s.fit(x_train, y_train)\n",
    "print('time', time.time() - st)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we should not focus solely on training time. In fact, whether its discovered features are useful or not is a more pressing concern. As a result, we build three decision models based on original features and those created by the evolutionary forest and gplearn, respectively.\n",
    "According to the experimental results, the feature obtained by the wrapper-based method can significantly improve the performance of the random forest model. However, the features obtained by the filter-based method do not appear to be as significant."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from evolutionary_forest.utils import get_feature_importance, feature_append\n",
    "\n",
    "\n",
    "def score_func(score_name, model_name, x_train, x_test):\n",
    "    scores = []\n",
    "    for i in range(10):\n",
    "        new_r = {\n",
    "            'RF': RandomForestRegressor(),\n",
    "            'AdaBoost': AdaBoostRegressor(),\n",
    "            'GBDT': GradientBoostingRegressor(subsample=0.8, n_estimators=100),\n",
    "            'DART': LGBMRegressor(boosting_type='DART', random_state=random.randint(0, 1000), n_estimators=100),\n",
    "        }[model_name]\n",
    "        new_r.fit(x_train, y_train)\n",
    "        score = r2_score(y_test, new_r.predict(x_test))\n",
    "        all_score.append((score_name, model_name, score))\n",
    "        scores.append(score)\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "code_importance_dict = get_feature_importance(r, simple_version=False)\n",
    "new_train_wrapper = feature_append(r, x_train, list(code_importance_dict.keys())[:50], only_new_features=True)\n",
    "new_test_wrapper = feature_append(r, x_test, list(code_importance_dict.keys())[:50], only_new_features=True)\n",
    "all_score = []\n",
    "\n",
    "wrapper_score = score_func('Wrapper', 'RF', new_train_wrapper, new_test_wrapper)\n",
    "print('r2_score', wrapper_score, np.mean(wrapper_score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "base_score = score_func('Base', 'RF', x_train, x_test)\n",
    "print('r2_score', base_score, np.mean(base_score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "new_train = s.transform(x_train)\n",
    "new_test = s.transform(x_test)\n",
    "filter_score = score_func('Filter', 'RF', new_train, new_test)\n",
    "print('r2_score', filter_score, np.mean(filter_score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To further solidify our conclusion, we must conduct a statistical test on the results. In this article, we will use the rank sum test to validate our assumption."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from scipy import stats\n",
    "\n",
    "print('base_score vs wrapper_score', stats.ranksums(base_score, wrapper_score).pvalue)\n",
    "print('base_score vs filter_score', stats.ranksums(base_score, filter_score).pvalue)\n",
    "print('wrapper_score vs filter_score', stats.ranksums(wrapper_score, filter_score).pvalue)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the statistical test, we can see that the best method, wrapper-based feature construction, outperforms its counterpart on this task significantly. Even though it costs twice as much, it is worthwhile to spend the extra time on this task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition, we run these generated features through a variety of machine learning algorithms, including AdaBoost, GBDT, and DART. Based on the final results, we conclude that the features generated by the wrapped-based method are more useful, which can effectively improve the model prediction performance. In contrast, while the filter-based method can improve model performance, the effect is minor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for model in ['AdaBoost', 'GBDT', 'DART']:\n",
    "    wrapper_score = score_func('Wrapper', model, new_train_wrapper, new_test_wrapper)\n",
    "    base_score = score_func('Base', model, x_train, x_test)\n",
    "    filter_score = score_func('Filter', model, new_train, new_test)\n",
    "    print(wrapper_score, base_score, filter_score)\n",
    "data = pd.DataFrame(all_score, columns=[\n",
    "    'Name',\n",
    "    'Model',\n",
    "    'Score',\n",
    "])\n",
    "sns.boxplot(x='Name', y='Score', hue='Model', data=data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, in this article, we look at two popular paradigms in feature construction: the wrapper-based feature construction method and the filter-based feature construction method. The former is more accurate, while the latter is faster. In fact, the purpose of this article is not to argue that we should abandon the filter-based feature construction method; each method has its own set of application scenarios. It is obvious that the filter-based method is better suited to an environment with limited computational resources because it takes less time. The main point of this article is that many people believe that the wrapper-based method is slow. However, we show in this article that such a method is perfectly acceptable in most cases. In the future, we should not consider the feature construction method to be an A or B problem, in my opinion. Perhaps weighing the benefits of both and identifying the most applicable scenarios is the best solution for the development of machine learning techniques."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
