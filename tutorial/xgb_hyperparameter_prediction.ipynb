{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Background\n",
    "\n",
    "In the field of machine learning, hyperparameter optimization is a critical task that has been studied by numerous researchers. One major challenge in HPO problems is predicting the predictive performance for unknown hyperparameters. In this article, we investigate whether PS-Forest is useful for the HPO performance prediction task and compare it to other algorithms such as XGBoost and Gaussian Process.\n",
    "\n",
    "### Dataset Preparation\n",
    "In our experiment, we first prepare experimental data by randomly sampling 1000 hyperparameter configurations from the HPO-Bench search space, and then we compute the validation score using the HPO-Bench training protocol. The real-world dataset used for training XGBoost is \"kr-vs-kp\". Our goal is to see if our algorithm can accurately predict the performance of new hyperparameters based on previous hyperparameter optimization data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/hengzhe-zhang/EvolutionaryForest/master/data/XGB_HPO_data.csv',\n",
    "                   index_col=0)\n",
    "for c in ['eta', 'min_child_weight', 'reg_alpha', 'reg_lambda']:\n",
    "    data[c] = np.log(data[c])\n",
    "\n",
    "x = np.array(data[['colsample_bylevel', 'colsample_bytree', 'eta', 'max_depth',\n",
    "                   'min_child_weight', 'reg_alpha',\n",
    "                   'reg_lambda', 'subsample_per_it']])\n",
    "y = np.array(data['function_value'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evolutionary Forest vs State-of-the-art Machine Learning Algorithms\n",
    "First, we can compare the test performance of evolutionary forest versus classical machine learning algorithms. The following results show that evolutionary forest achieves a testing $R^2$ score up to 0.82, while the best base line model (ET) only achieves a test score of 0.81."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "from evolutionary_forest.forest import EvolutionaryForestRegressor\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def numba_seed(a):\n",
    "    random.seed(a)\n",
    "    np.random.seed(a)\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=0)\n",
    "model = EvolutionaryForestRegressor(max_height=3, min_height=1,\n",
    "                                    normalize=False, select='AutomaticLexicase', ensemble_size=100,\n",
    "                                    basic_primitives='add,subtract,multiply,analytical_quotient,protect_sqrt,analytical_loge,maximum',\n",
    "                                    mutation_scheme='EDA-Terminal-PM',\n",
    "                                    semantic_diversity='GreedySelection-Resampling', initial_tree_size='1-2',\n",
    "                                    cross_pb=0.9, mutation_pb=0.1, gene_num=10, n_gen=30,\n",
    "                                    n_pop=200, base_learner='Fast-RidgeDT')\n",
    "ef = model.fit(x_train, y_train)\n",
    "print('Testing Score (EF)', r2_score(y_test, ef.predict(x_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "scores_base = []\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=0)\n",
    "data_columns = ['CBL', 'CBT', 'ETA', 'MD',\n",
    "                'MCW', 'RA', 'RL', 'SS']\n",
    "models = ['Ridge', 'GPR', 'RF', 'ET', 'AdaBoost', 'GBDT', 'XGBoost', 'LightGBM']\n",
    "for model_name in models:\n",
    "    model = {\n",
    "        'Ridge': Ridge(),\n",
    "        'GPR': Pipeline([\n",
    "            ('Scaler', StandardScaler()),\n",
    "            ('GP', GaussianProcessRegressor(kernel=Matern(), normalize_y=True))\n",
    "        ]),\n",
    "        'RF': RandomForestRegressor(n_estimators=100),\n",
    "        'ET': ExtraTreesRegressor(n_estimators=100),\n",
    "        'AdaBoost': AdaBoostRegressor(n_estimators=100),\n",
    "        'GBDT': GradientBoostingRegressor(n_estimators=100),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, n_jobs=1),\n",
    "        'LightGBM': LGBMRegressor(n_estimators=100, n_jobs=1),\n",
    "    }[model_name]\n",
    "\n",
    "    model = model.fit(x_train, y_train)\n",
    "    base_score = r2_score(y_test, model.predict(x_test))\n",
    "    print(model_name, 'Testing Score (Original Features)', base_score)\n",
    "    scores_base.append((model_name, base_score))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Importance Map\n",
    "\n",
    "However, only achieve the best accuracy is not enough. We hope to gain more insights in addition to achieve the excellent performance. Fortunately, we can obtain a feature importance map from Evolutionary Forest.\n",
    "\n",
    "Based on the feature importance map, it is clear that features $Max(MCW,RA)$ and $\\frac{Max(MCW,RA)}{SS}$ are very important for evolutionary forest. These features are used to control the magnitude regularization. Thus, it is no wonder there features are important in predicting the performance of EF.\n",
    "\n",
    "Nonetheless, on interesting thing is that evolutionary forest is able to discover a high order feature $Max(MCW,RA)$. This term means that the maximum of min_child_weight and reg_alpha will determine the generalization performance of XGBoost. In fact, this is perfectly true. If either min_child_weight or reg_alpha is large, the remaining term will not have an impact on regularization. Thus, the maximum of these two terms is useful for predicting the predictive performance of XGBoost."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from evolutionary_forest.utils import get_feature_importance, plot_feature_importance\n",
    "\n",
    "\"\"\"\n",
    "Mapping:\n",
    "{'colsample_bylevel': 'CBL',\n",
    " 'colsample_bytree': 'CBT',\n",
    " 'eta': 'ETA',\n",
    " 'max_depth': 'MD',\n",
    " 'min_child_weight': 'MCW',\n",
    " 'reg_alpha': 'RA',\n",
    " 'reg_lambda': 'RL',\n",
    " 'subsample_per_it': 'SS'}\n",
    "\"\"\"\n",
    "feature_importance_dict = get_feature_importance(ef, latex_version=True)\n",
    "for k, v in list(feature_importance_dict.items()):\n",
    "    del feature_importance_dict[k]\n",
    "    for i in range(len(data_columns) - 1, -1, -1):\n",
    "        k = k.replace(\"X_{%d}\" % i, data_columns[i])\n",
    "    feature_importance_dict[k] = v\n",
    "plot_feature_importance(feature_importance_dict, save_fig=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying constructed features to the model\n",
    "Based on the above analysis, we add two constructed features to our data, and then re-train previous machine learning models. The following experimental results show that only adding these two features can improve the predictive performance on all models, including tree-based models and linear regression models. Based on these results, we can conclude that evolutionary forest is a promising method not only for getting a better prediction model, but also helpful for getting some insights to improve existing machine learning models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "scores_enhanced = []\n",
    "\n",
    "for model_name in models:\n",
    "    model = {\n",
    "        'Ridge': Ridge(),\n",
    "        'GPR': Pipeline([\n",
    "            ('Scaler', StandardScaler()),\n",
    "            ('GP', GaussianProcessRegressor(kernel=Matern(), normalize_y=True))\n",
    "        ]),\n",
    "        'RF': RandomForestRegressor(n_estimators=100),\n",
    "        'ET': ExtraTreesRegressor(n_estimators=100),\n",
    "        'AdaBoost': AdaBoostRegressor(n_estimators=100),\n",
    "        'GBDT': GradientBoostingRegressor(n_estimators=100),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, n_jobs=1),\n",
    "        'LightGBM': LGBMRegressor(n_estimators=100, n_jobs=1),\n",
    "    }[model_name]\n",
    "\n",
    "    c1 = np.max([x[:, data_columns.index('MCW')], x[:, data_columns.index('RA')]], axis=0)\n",
    "    c2 = x[:, data_columns.index('MCW')] / x[:, data_columns.index('SS')]\n",
    "    x_enhanced = np.concatenate([x, c1.reshape(-1, 1), c2.reshape(-1, 1)], axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_enhanced, y, test_size=0.9, random_state=0)\n",
    "    model = model.fit(x_train, y_train)\n",
    "    enhanced_score = r2_score(y_test, model.predict(x_test))\n",
    "    print('Testing Score (Constructed Features)', enhanced_score)\n",
    "    scores_enhanced.append((model_name, enhanced_score))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Score aggregation\n",
    "base_frame = pd.DataFrame(scores_base, columns=['Algorithm', 'Score'])\n",
    "base_frame['Score'] = pd.to_numeric(base_frame['Score'])\n",
    "print(base_frame.groupby(['Algorithm']).mean())\n",
    "improved_frame = pd.DataFrame(scores_enhanced, columns=['Algorithm', 'Score'])\n",
    "improved_frame['Score'] = pd.to_numeric(improved_frame['Score'])\n",
    "print(improved_frame.groupby(['Algorithm']).mean())\n",
    "base_score = base_frame.groupby(['Algorithm']).mean()\n",
    "improved_score = improved_frame.groupby(['Algorithm']).mean()\n",
    "print(improved_score - base_score)\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.set_palette('viridis')\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(base_score.index, base_score['Score'], width, label='Without Constructed Features')\n",
    "difference = improved_score['Score'] - base_score['Score']\n",
    "print(np.where(difference > 0, 'g', 'y'))\n",
    "ax.bar(base_score.index, difference, width, bottom=base_score['Score'],\n",
    "       label='With Constructed Features',\n",
    "       color=np.where(difference > 0, 'r', 'y'))\n",
    "ax.set_ylabel('Score ($R^2$)')\n",
    "ax.set_title('Effect of Feature Construction')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Constructed Features for Learning To Rank\n",
    "In AutoML domain, some experts may prefer learning to rank (LTR) technique rather than regression technique because they claim that the relative order of different hyperparameter settings is more important than the specific value. This is an emerging trend, but still have not been fully adopted. In many AutoML algorithms, traditional regression techniques, especially Gaussian Process, is still widely adopted.\n",
    "\n",
    "No matter what, our constructed feature can also be used for enhancing LTR model. Here, we try to construct a LTR model based on XGB. This XGB model is different from the previous one, because the objective function of this model is pairwise loss instead of mean squared error, which is more suitable for LTR.\n",
    "\n",
    "From the experimental results, we can see that the spearman correlation coefficient of the model constructed from original features is 0.87. After adding two constructed features, it increases to 0.90, which demonstrates the validity of feature construction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "model = XGBRegressor(objective='rank:pairwise', n_estimators=100, n_jobs=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=0)\n",
    "model = model.fit(x_train, y_train)\n",
    "print('XGBoost', 'Testing Spearman Score (Original Features)', spearmanr(y_test,model.predict(x_test))[0])\n",
    "print('XGBoost', 'Testing Kendall Score (Original Features)', kendalltau(y_test,model.predict(x_test))[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "c1 = np.max([x[:, data_columns.index('MCW')], x[:, data_columns.index('RA')]], axis=0)\n",
    "c2 = x[:, data_columns.index('MCW')] / x[:, data_columns.index('SS')]\n",
    "x_enhanced = np.concatenate([x, c1.reshape(-1, 1), c2.reshape(-1, 1)], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_enhanced, y, test_size=0.9, random_state=0)\n",
    "model = model.fit(x_train, y_train)\n",
    "print('XGBoost', 'Testing Spearman Score (Original Features)', spearmanr(y_test,model.predict(x_test))[0])\n",
    "print('XGBoost', 'Testing Kendall Score (Original Features)', kendalltau(y_test,model.predict(x_test))[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
